from __future__ import annotations

import argparse
from collections.abc import Iterable, Mapping, Sequence
from dataclasses import dataclass
import json
from pathlib import Path
import sys
from typing import Any

# Import only lightweight helpers at module import time
from physical_ai_stl.monitoring.rtamt_monitor import (
    evaluate_series as _rtamt_evaluate_series,
    satisfied as _rtamt_satisfied,
    stl_always_upper_bound as _rtamt_stl_upper,
)

# -----------------------------------------------------------------------------
# Argument parsing
# -----------------------------------------------------------------------------

@dataclass
class Args:
    ckpt: str
    var: str
    semantics: str          # 'dense' or 'discrete'
    dt: float | None
    agg: str                # spatial reducer
    p: float                # lp norm (for --agg lp)
    q: float                # quantile (for --agg quantile)
    temp: float             # temperature (for --agg softmax)
    spec: str               # STL spec string or path to file
    json_out: str | None
    device: str | None
    batch_size: int
    backend: str            # 'rtamt' (others may be added later)
    seed: int | None
    n: int | None           # number of trajectories to evaluate (if dataset sampled)
    tmin: float | None      # start time for monitoring window
    tmax: float | None      # end time for monitoring window
    time_col: str           # name for time column in output
    quiet: bool             # suppress some notes


def _build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=(
            "Evaluate an STL specification on trajectories generated by a diffusion "
            "model checkpoint and summarize robustness / satisfaction using RTAMT."
        )
    )
    parser.add_argument(
        "--ckpt",
        dest="ckpt",
        type=str,
        required=True,
        help="Path to diffusion model checkpoint (directory or file).",
    )
    parser.add_argument(
        "--var",
        dest="var",
        type=str,
        required=True,
        help="Name of the scalar field/variable to monitor (e.g., 'u').",
    )
    parser.add_argument(
        "--semantics",
        dest="semantics",
        type=str,
        choices=("dense", "discrete"),
        default="dense",
        help="STL semantics for time traces (default: dense).",
    )
    parser.add_argument(
        "--dt",
        dest="dt",
        type=float,
        default=None,
        help=(
            "Time-step for discrete-time monitoring. If omitted, will be inferred "
            "from timestamps when possible."
        ),
    )
    parser.add_argument(
        "--agg",
        dest="agg",
        type=str,
        choices=("max", "min", "mean", "median", "lp", "quantile", "softmax"),
        default="max",
        help=(
            "Spatial aggregation reducer over grid points (default: max). "
            "Use --p/--q/--temp to parameterize lp/quantile/softmax."
        ),
    )
    parser.add_argument(
        "--p",
        dest="p",
        type=float,
        default=2.0,
        help="p for lp aggregation (only used when --agg=lp).",
    )
    parser.add_argument(
        "--q",
        dest="q",
        type=float,
        default=0.5,
        help="quantile in [0,1] for quantile aggregation (only used when --agg=quantile).",
    )
    parser.add_argument(
        "--temp",
        dest="temp",
        type=float,
        default=1.0,
        help="temperature for softmax aggregation (only used when --agg=softmax).",
    )
    parser.add_argument(
        "--spec",
        dest="spec",
        type=str,
        required=True,
        help=(
            "STL specification string or a path to a file containing the spec. "
            "Examples: 'always[0,5](x < 0.2)' or 'specs/safety.stl'."
        ),
    )
    parser.add_argument(
        "--json-out",
        dest="json_out",
        type=str,
        default=None,
        help="Optional path to write a JSON summary",
    )
    parser.add_argument(
        "--device",
        dest="device",
        type=str,
        default=None,
        help="Torch device for model sampling (e.g., 'cpu', 'cuda:0').",
    )
    parser.add_argument(
        "--batch-size",
        dest="batch_size",
        type=int,
        default=16,
        help="Batch size when sampling trajectories from the model.",
    )
    parser.add_argument(
        "--backend",
        dest="backend",
        type=str,
        default="rtamt",
        choices=("rtamt",),
        help="STL monitoring backend to use (default: rtamt).",
    )
    parser.add_argument(
        "--seed",
        dest="seed",
        type=int,
        default=None,
        help="Random seed for reproducibility.",
    )
    parser.add_argument(
        "--n",
        dest="n",
        type=int,
        default=None,
        help="Optional number of trajectories to evaluate (dataset/sample dependent).",
    )
    parser.add_argument(
        "--tmin",
        dest="tmin",
        type=float,
        default=None,
        help="Start time for the monitoring window (inclusive).",
    )
    parser.add_argument(
        "--tmax",
        dest="tmax",
        type=float,
        default=None,
        help="End time for the monitoring window (inclusive).",
    )
    parser.add_argument(
        "--time-col",
        dest="time_col",
        type=str,
        default="t",
        help="Column name used for time when exporting or printing traces.",
    )
    parser.add_argument(
        "--quiet",
        dest="quiet",
        action="store_true",
        help="Reduce informational console output.",
    )
    return parser


# -----------------------------------------------------------------------------
# Utilities
# -----------------------------------------------------------------------------

def _read_spec(spec_arg: str) -> str:
    """
    Read an STL specification from a string or file path.

    If *spec_arg* is a path to an existing file, read the file contents,
    otherwise return *spec_arg* unchanged.
    """
    p = Path(spec_arg)
    if p.exists():
        return p.read_text(encoding="utf-8").strip()
    return spec_arg


def _resolve_device(device: str | None) -> str:
    """
    Resolve a requested device string to a torch device.

    Falls back to CPU if CUDA is unavailable (or torch is not installed).
    """
    if device is None:
        try:
            import torch  # local import to avoid hard dependency at import-time
            return "cuda" if torch.cuda.is_available() else "cpu"
        except Exception:
            return "cpu"
    if device.startswith("cuda"):
        try:
            import torch
            if torch.cuda.is_available():
                return device
            return "cpu"
        except Exception:
            return "cpu"
    if device == "auto":
        import torch  # type: ignore
        return "cuda" if torch.cuda.is_available() else "cpu"
    return device


def _set_seed(seed: int | None) -> None:
    if seed is None:
        return
    try:
        import torch
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    except Exception:
        pass


def _spatial_reduce(x: "torch.Tensor", how: str, p: float, q: float, temp: float) -> "torch.Tensor":
    """
    Spatially reduce tensor *x* along spatial axes using method *how*.

    Supported:
      - "max", "min", "mean", "median"
      - "lp" (with p)
      - "quantile" (with q in [0,1])
      - "softmax" (soft-aggregation with temperature)
    """
    import torch

    if how == "max":
        return x.amax(dim=tuple(range(2, x.ndim)))
    if how == "min":
        return x.amin(dim=tuple(range(2, x.ndim)))
    if how == "mean":
        return x.mean(dim=tuple(range(2, x.ndim)))
    if how == "median":
        return x.median(dim=tuple(range(2, x.ndim))).values
    if how == "lp":
        if p <= 0:
            raise ValueError("lp requires p > 0")
        # ||x||_p over spatial dims
        return (x.abs().pow(p).mean(dim=tuple(range(2, x.ndim))).pow(1.0 / p))
    if how == "quantile":
        if not (0.0 <= q <= 1.0):
            raise ValueError("quantile requires q in [0,1]")
        return x.quantile(q, dim=tuple(range(2, x.ndim)))
    if how == "softmax":
        # softmax over spatial dims, then weighted sum
        dims = tuple(range(2, x.ndim))
        w = (x / max(temp, 1e-12)).softmax(dim=dims[0])
        for d in dims[1:]:
            w = w.softmax(dim=d)
        return (w * x).sum(dim=dims)
    raise ValueError(f"Unknown spatial reducer: {how}")


def _to_numpy_trace(
    y: "torch.Tensor",
    t: "torch.Tensor",
    var: str,
    time_col: str,
    *,
    agg: str,
    p: float,
    q: float,
    temp: float,
) -> tuple[list[float], Mapping[str, list[float]]]:
    """
    Convert a batch of trajectories *y* with timestamps *t* to a trace format suitable
    for the monitoring backend.

    Returns:
      times, dict(var -> series)
    """
    import torch

    # y: (B, T, *spatial)
    # t: (B, T)
    if y.ndim < 2:
        raise ValueError("Expected y with shape (B, T, ...)")
    if t.ndim != 2 or t.shape[0] != y.shape[0] or t.shape[1] != y.shape[1]:
        raise ValueError("t should have shape (B, T) matching y's batch/time dims")

    # Reduce over spatial dimensions if any
    if y.ndim > 2:
        ys = _spatial_reduce(y, agg, p, q, temp)  # (B, T)
    else:
        ys = y  # (B, T)

    # Assume batch dimension is independent traces; concatenate
    times: list[float] = []
    series: list[float] = []
    for b in range(ys.shape[0]):
        tb = t[b].detach().cpu().numpy().tolist()
        yb = ys[b].detach().cpu().numpy().tolist()
        times.extend(tb)
        series.extend(yb)

    return times, {var: series} | {time_col: times}  # include time column for CSV friendliness


def _infer_dt(times: Sequence[float]) -> float | None:
    """Infer an approximate dt for discrete-time semantics if possible."""
    if not times:
        return None
    import numpy as np

    arr = np.asarray(times, dtype=float)
    if arr.size < 2:
        return None
    diffs = np.diff(arr)
    # Heuristic: if diffs are fairly uniform, take the mean; otherwise, warn.
    dt = float(diffs.mean())
    if (diffs.std() / (abs(dt) + 1e-12)) < 1e-3:
        return dt
    return dt


def _evaluate_with_rtamt(
    times: Sequence[float],
    signals: Mapping[str, Sequence[float]],
    spec: str,
    *,
    semantics: str,
    dt: float | None,
) -> tuple[float, bool]:
    """
    Evaluate *spec* on the provided trace using RTAMT.

    Returns (robustness, satisfied).
    """
    if semantics not in {"dense", "discrete"}:
        raise ValueError("semantics must be 'dense' or 'discrete'")

    if semantics == "discrete":
        # Ensure dt exists for discrete-time semantics
        if dt is None:
            dt = _infer_dt(times)
        if dt is None:
            raise ValueError(
                "Discrete semantics require a time-step dt; provide --dt or use dense semantics."
            )
    rob = _rtamt_evaluate_series(
        series=signals,
        times=times,
        spec=spec,
        semantics=semantics,
        dt=dt,
    )
    sat = _rtamt_satisfied(robustness=rob)
    return float(rob), bool(sat)


def _load_and_sample(
    ckpt: str,
    *,
    device: str,
    n: int | None,
    batch_size: int,
) -> tuple["torch.Tensor", "torch.Tensor"]:
    """
    Load a diffusion model checkpoint and sample trajectories.

    Returns:
      y: (B, T, *spatial)
      t: (B, T)
    """
    import torch

    # This function is intentionally minimal and avoids hard-coding any framework specifics.
    # Expectation: the checkpoint directory/file contains a torchscript or state-dict loadable
    # object exposing a `.sample(n, device, batch_size)` method returning (y, t).
    path = Path(ckpt)
    if not path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {path}")

    # User-provided callable in checkpoint (duck-typed)
    try:
        obj = torch.load(path, map_location=device)
    except Exception as e:
        raise RuntimeError(f"Unable to load checkpoint {path}: {e}") from e

    if hasattr(obj, "sample") and callable(getattr(obj, "sample")):
        y, t = obj.sample(n=n, device=device, batch_size=batch_size)
        if not (isinstance(y, torch.Tensor) and isinstance(t, torch.Tensor)):
            raise TypeError("sample() must return (Tensor y, Tensor t)")
        return y.to(device), t.to(device)

    # Fallback: obj is a dict containing tensors y and t
    if isinstance(obj, dict) and "y" in obj and "t" in obj:
        y = obj["y"]
        t = obj["t"]
        if not (isinstance(y, torch.Tensor) and isinstance(t, torch.Tensor)):
            raise TypeError("Checkpoint dict must contain tensors 'y' and 't'")
        if device:
            y = y.to(device)
            t = t.to(device)
        return y, t

    raise RuntimeError(
        "Unsupported checkpoint format. Provide an object with .sample(...) or a dict with 'y' and 't'."
    )


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------

def parse_args(argv: Sequence[str] | None = None) -> Args:
    parser = _build_parser()
    ns = parser.parse_args(argv)

    args = Args(
        ckpt=ns.ckpt,
        var=ns.var,
        semantics=ns.semantics,
        dt=ns.dt,
        agg=ns.agg,
        p=ns.p,
        q=ns.q,
        temp=ns.temp,
        spec=_read_spec(ns.spec),
        json_out=ns.json_out,
        device=ns.device,
        batch_size=ns.batch_size,
        backend=ns.backend,
        seed=ns.seed,
        n=ns.n,
        tmin=ns.tmin,
        tmax=ns.tmax,
        time_col=ns.time_col,
        quiet=ns.quiet,
    )
    return args


def main(argv: Sequence[str] | None = None) -> None:
    args = parse_args(argv)

    if args.backend != "rtamt":
        raise NotImplementedError(f"Only --backend=rtamt is currently supported (got {args.backend})")

    device = _resolve_device(args.device)
    _set_seed(args.seed)

    if not args.quiet:
        print(f"[info] device={device}", file=sys.stderr)

    # Load and sample trajectories
    y, t = _load_and_sample(
        args.ckpt,
        device=device,
        n=args.n,
        batch_size=args.batch_size,
    )

    # Convert to trace format
    times, signals = _to_numpy_trace(
        y=y,
        t=t,
        var=args.var,
        time_col=args.time_col,
        agg=args.agg,
        p=args.p,
        q=args.q,
        temp=args.temp,
    )

    # Monitoring window
    if args.tmin is not None or args.tmax is not None:
        tmin = -float("inf") if args.tmin is None else float(args.tmin)
        tmax = float("inf") if args.tmax is None else float(args.tmax)
        mask = [tmin <= ti <= tmax for ti in times]
        times = [ti for ti, m in zip(times, mask) if m]
        signals = {
            k: [vi for vi, m in zip(v, mask) if m] for k, v in signals.items()
        }

    # Infer dt if needed (discrete semantics)
    dt = args.dt
    if args.semantics == "discrete" and dt is None:
        diffs = None
        try:
            import numpy as np

            arr = np.asarray(times, dtype=float)
            if arr.size >= 2:
                diffs = np.diff(arr)
                dt = float(diffs.mean())
        except Exception:
            dt = None

        if dt is None:
            raise RuntimeError(
                "Failed to infer dt from timestamps; provide --dt to use discrete semantics."
            )

        if not args.quiet and diffs is not None:
            print(
                f"[note] Non‑uniform time grid (σ={diffs.std():.3g}); using mean dt={dt:.6g}",
                file=sys.stderr,
            )

    # Evaluate spec
    if args.backend == "rtamt":
        backend = "rtamt"
        rob, sat = _evaluate_with_rtamt(
            times=times,
            signals=signals,
            spec=args.spec,
            semantics=args.semantics,
            dt=dt,
        )
    else:
        raise NotImplementedError(args.backend)

    # Print results
    nt = len(times)
    print(f"robustness: {rob:.6g}")
    print(f"satisfied: {bool(sat)}")
    print(f"n_samples: {nt}")
    print(f"backend: {backend}")

    # Upper bound under always operator (simple helper)
    try:
        ub = _rtamt_stl_upper(args.spec)
        if not args.quiet:
            print(f"[info] upper bound under always(): {ub:.6g}", file=sys.stderr)
    except Exception:
        ub = None

    # Optional JSON summary
    if args.json_out:
        summary: dict[str, Any] = {
            "ckpt": args.ckpt,
            "var": args.var,
            "semantics": args.semantics,
            "dt": dt,
            "agg": args.agg,
            "p": args.p,
            "q": args.q,
            "temp": args.temp,
            "spec": args.spec,
            "n_samples": nt,
            "robustness": rob,
            "satisfied": bool(sat),
            "backend": backend,
        }
        outp = Path(args.json_out)
        outp.parent.mkdir(parents=True, exist_ok=True)
        with outp.open("w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2)
        print(f"[saved] {outp}")


if __name__ == "__main__":
    main()
