# Roadmap & Check‑ins — Fall 2025

> **Course:** Vanderbilt **CS‑3860‑01 Undergraduate Research** (3 credits) · **Instructor:** Prof. Taylor T. Johnson · **Student:** Arthur Hu  
> **Focus:** *Physical AI × (Spatial) Signal Temporal Logic (STL/STREL)* — monitor / softly enforce temporal & spatio‑temporal specifications in physics‑ML (neural ODE/PDE) models.  
> **Standing meeting:** **Fridays 11:00** (adjust as the lab finalizes schedule; Zoom during construction).  
> **Effort target:** **6–9 hrs/week** (rule of thumb for 3 credits); **deliver a report** by semester end.

---

## 0) Objectives & deliverables (what “done” means)

**Primary objective.** Build a **lean, reproducible** experimental stack that:
1) evaluates **physics‑ML frameworks** for PDE/ODE modeling on small demos;  
2) integrates **STL/STREL monitoring** (time + space) for **training‑time penalties** and **evaluation**;  
3) yields **evidence‑backed recommendations** on frameworks, datasets, and spec patterns.

**Deliverables (graded artifacts).**
- 📘 **End‑of‑semester report** (10–12 pages) — methods, results, trade‑offs, and recommendations. See `docs/report/outline.md` for structure.
- 📂 **Open‑source repo** — runnable scripts, configs, seeds, and brief docs; CPU‑first (GPU optional). CI runs smoke tests.
- 🧪 **Reproducible experiments** — ablations + figures; exact commands logged.
- 🗒️ **Sprint notes** — brief bi‑weekly summaries (`docs/sprint*_report.md`).

**Definition of Done (per experiment).**
- Tracked configs/seeds; results saved to `runs/`.  
- Training curves + **robustness (ρ)** curves; aggregate metrics; plots generated by `scripts/plot_ablations.py`.  
- **Spec library** (STL/STREL) with plain‑English paraphrases and unit tests.  
- “What we learned” bullet list (1–2 paragraphs) + compute budget.

---

## 1) Scope & stacks (what we will compare)

### Physics‑ML frameworks (evaluate & pick one primary)
- **Neuromancer** — PyTorch differentiable programming for **parametric constrained optimization**, physics‑informed ID, and **differentiable predictive control**. Good docs; easy to pipe custom losses/monitors.  ([GitHub](https://github.com/pnnl/neuromancer), [Docs](https://pnnl.github.io/neuromancer/))  
- **NVIDIA PhysicsNeMo** — state‑of‑the‑art **SciML** pipelines; strong **neural operators/PINNs**, scalable training; heavier install; GPU‑leaning but has CPU paths.  ([GitHub](https://github.com/NVIDIA/physicsnemo))  
- **Bosch TorchPhysics** — light PyTorch library for **PINN/DeepRitz/DeepONet/FNO** PDEs; very fast to stand up new PDEs.  ([GitHub](https://github.com/boschresearch/torchphysics), [Docs](https://boschresearch.github.io/torchphysics/))

**Rationale.** Start with **Neuromancer** as primary (fast PyTorch loop + constraints); keep **TorchPhysics** for PDE variety; treat **PhysicsNeMo** as stretch when scaling operators or GPU. See `docs/framework_survey.md` for the full comparison.

### STL/STREL monitoring (time & space)
- **RTAMT** — STL robustness monitors; **offline & online (bounded‑future)**; discrete & dense time; Python with optional C++ backend.  ([GitHub](https://github.com/nickovic/rtamt), 2025 overview: [arXiv](https://arxiv.org/abs/2501.18608))  
- **MoonLight (STREL)** — spatio‑temporal **reach/escape** logic; Java engine with Python/Matlab interfaces; Python wrapper used for 2D fields.  ([GitHub](https://github.com/MoonLightSuite/moonlight), [Paper](https://arxiv.org/abs/2104.14333), [Journal](https://link.springer.com/article/10.1007/s10009-023-00710-5))  
- **SpaTiaL** — object‑centric spatial relations + simple planning; optional robotics‑style demos.  ([GitHub org](https://github.com/KTH-RPL-Planiacs), [API docs](https://kth-rpl-planiacs.github.io/SpaTiaL/))

### Prior art to emulate (loss shaping)
- **STLnet (NeurIPS 2020)** — uses **robust STL semantics** as differentiable guidance for sequence models; inspires our “logic‑as‑loss” approach.  ([Paper](https://proceedings.neurips.cc/paper/2020/file/a7da6ba0505a41b98bd85907244c4c30-Paper.pdf), [Abstract](https://proceedings.neurips.cc/paper/2020/hash/a7da6ba0505a41b98bd85907244c4c30-Abstract.html))

---

## 2) Datasets / problem spaces (STL‑ready)

We prioritize **small, CPU‑friendly** tasks with clear temporal or spatio‑temporal properties. Full details and more options in `docs/dataset_recommendations.md`.

**Pilot (T1).**
- **1D diffusion / 2D heat (synthetic)** — already scaffolded here; train fast; ideal for wiring end‑to‑end STL/STREL.  
  *Spec ideas:* safety bound `G_[0,T](u≤U_max)`, eventual cooling `F_[0,τ]G_[0,T](u≤U_safe)`, STREL containment of a hotspot region.  
- **Burgers’ 1D (synthetic)** — nonlinearity adds challenge; use TorchPhysics.

**Tier‑2.**
- **Air quality (UCI Beijing multi‑site)** — multivariate time series with known temporal patterns; STL over thresholds and recovery.  
- **Traffic speeds (METR‑LA / PEMS‑BAY)** — grid‑like spatio‑temporal signals; test STREL reach/escape (congestion spread).

**Stretch.** PDEBench minis (advection‑diffusion) or small **Navier–Stokes (FNO)** slices when GPU is available.

---

## 3) Integration plan (how specs enter training & eval)

1) **Monitoring.** Wrap RTAMT/MoonLight monitors with simple Python adapters:  
   `ρ = monitor(formula, signal)` returning **robustness** (≥0 satisfy, <0 violate).  
2) **Loss shaping (soft enforcement).** Add **differentiable robustness** penalties:  
   - **Max‑margin:** `L_spec = ReLU(−ρ + ε)`;  
   - **Smoothing:** replace `min/max/∨/∧` with **softmin/softmax** (temperature τ) for gradient flow;  
   - **Schedule:** increase weight `λ_spec` and sharpen τ over epochs.  
   *Inspired by STLnet.*
3) **Constraint optimization (optional).** Use augmented Lagrangian / penalty in **Neuromancer** to treat specs as soft constraints.  
4) **Evaluation.** Report **spec satisfaction rate** and **average robustness margin** (higher is better) on train/val/test; run **online** (bounded‑future) monitors where relevant; keep offline evaluation as ground truth.  
5) **Spatial signals.** For 2D fields, map grids to MoonLight graphs (cells → nodes; adjacency → 4/8‑neighborhood with distances).

---

## 4) Milestones & calendar (week‑by‑week)

> Dates use Monday–Sunday windows starting **Oct 06, 2025**. We’ll adapt as needed; meet **Fridays 11:00**.

| Week | Dates | Goals & tangible outcomes |
| --- | --- | --- |
| **W1** | Oct 06–Oct 12 | ✅ Set up env (CPU‑first); verify `scripts/check_env.py`. Reproduce **1D diffusion** w/ baseline loss; implement **RTAMT eval** (`scripts/eval_diffusion_rtamt.py`). Document two STL formulas. |
| **W2** | Oct 13–Oct 19 | Wire **differentiable STL loss** (softmin/softmax) into `train_diffusion_stl.py`; ablate `λ_spec, τ`. Add plots via `scripts/plot_ablations.py`. |
| **W3** | Oct 20–Oct 26 | Bring up **2D heat + STREL** (`train_heat2d_strel.py`); define hotspot containment spec (`scripts/specs/contain_hotspot.mls`). Export MoonLight Python adapter + unit tests. |
| **W4** | Oct 27–Nov 02 | Expand to **TorchPhysics Burgers 1D** (`train_burgers_torchphysics.py`); align monitoring API; add eval script for Burgers. |
| **W5** | Nov 03–Nov 09 | **Framework checkpoint** — run matching tasks in **Neuromancer** and **TorchPhysics**; record **accuracy vs robustness vs time/memory**. Draft the **Frameworks** section of the report. |
| **W6** | Nov 10–Nov 16 | **Candidate real data**: air quality (UCI) or small traffic slice. Define 2–3 STL patterns; run offline monitoring; prototype one training run (if time). |
| **W7** | Nov 17–Nov 23 | **Ablations**: vary spec sharpness (τ), penalty schedules, and noise. Generate Pareto curves (**task loss** ↔ **robustness**). |
| **W8** | Nov 24–Nov 30 | **Stress tests**: OOD IC/BC, grid resolution changes. Start drafting **Results** + **Discussion**. (Short week—Thanksgiving.) |
| **W9** | Dec 01–Dec 07 | **Polish figures**, finalize narrative; rerun any flaky seeds; prepare **artifact instructions**. |
| **W10** | Dec 08–Dec 14 | **Final report** & repository freeze (tags, checksums). Dry‑run reproducibility script; submit and present. |

**Check‑ins.** Brief agenda posted before Friday: blockers, decisions needed, next‑week plan. Keep a running log at the bottom of this file.

---

## 5) Metrics, efficiency, and reproducibility

**Metrics (report all with 95% CIs across ≥3 seeds).**
- Task error (L2/MAE); **robustness margin** (mean/median ρ); **% satisfied**; compute **costs** (wall time, peak RAM/VRAM).  
- For spatial tasks: region‑level robustness and **front width** / **containment radius**.

**Speed & resource tips (applied by default).**
- Prefer **CPU‑friendly demos** (32‑bit floats, small grids: 1D ≤ 256, 2D ≤ 64×64); vectorize; avoid Python loops in monitors.  
- **Cache** static operators (Laplacian kernels, neighbor indices). Pre‑allocate tensors; turn off grad where not needed.  
- Use PyTorch **amp** on GPU when available; otherwise set `torch.set_num_threads(k)` to avoid oversubscription.  
- Keep optional stacks behind `requirements-extra.txt`; lazy‑import Java/Python bridges for MoonLight.  
- Continuous profiling: log epoch time & memory.

**Reproducibility.**
- Exact **env file** + pin optional deps; deterministic seeds; save **configs, checkpoints, and monitors** with versions.  
- CI runs a CPU smoke test (diffusion/heat) to catch regressions. See `docs/REPRODUCIBILITY.md`.

---

## 6) Risks & fallbacks

- **MoonLight friction (Java/Python bindings).** Fallback: STREL‑like neighborhood specs via pure‑PyTorch masks; or use MoonLight **offline** only.  
- **Online vs differentiability limits.** If online monitors break differentiability, keep **offline ground truth** and train with **smooth surrogates**.  
- **PhysicsNeMo install or compute heavy.** Keep as **evaluation‑only** later; focus on Neuromancer/TorchPhysics.  
- **Real‑data surprises.** If air‑quality/traffic wrangling stalls, stay on synthetic PDEs and document transfer to real data.

---

## 7) Backlog (living)

- [ ] Add **traffic** toy grid + STREL spec (congestion not escaping ring).  
- [ ] Add **Gray–Scott** reaction‑diffusion mini demo.  
- [ ] Try **augmented Lagrangian** spec constraints in Neuromancer.  
- [ ] Explore **neural operators** (FNO/UNO) via PhysicsNeMo/TorchPhysics for faster PDE fields.  
- [ ] Draft a **dataset card** template for each task (signals, specs, eval).

---

## 8) Pointers (quick links)

- **Frameworks:** Neuromancer ([GH](https://github.com/pnnl/neuromancer), [Docs](https://pnnl.github.io/neuromancer/)); PhysicsNeMo ([GH](https://github.com/NVIDIA/physicsnemo)); TorchPhysics ([GH](https://github.com/boschresearch/torchphysics), [Docs](https://boschresearch.github.io/torchphysics/)).  
- **STL/STREL:** RTAMT ([GH](https://github.com/nickovic/rtamt), [arXiv 2025](https://arxiv.org/abs/2501.18608)); MoonLight/STREL ([GH](https://github.com/MoonLightSuite/moonlight), [arXiv 2021](https://arxiv.org/abs/2104.14333), [Journal 2023](https://link.springer.com/article/10.1007/s10009-023-00710-5)); SpaTiaL ([GH org](https://github.com/KTH-RPL-Planiacs), [API](https://kth-rpl-planiacs.github.io/SpaTiaL/)).  
- **Prior art:** STLnet (NeurIPS 2020) ([paper](https://proceedings.neurips.cc/paper/2020/file/a7da6ba0505a41b98bd85907244c4c30-Paper.pdf), [abstract](https://proceedings.neurips.cc/paper/2020/hash/a7da6ba0505a41b98bd85907244c4c30-Abstract.html)).

---

## Running log (free‑form)

*Copy the template below for each week.*

**Date:** …  
**Focus:** …  
**What worked / blocked:** …  
**Next:** …

**Last updated:** Sep 30, 2025
